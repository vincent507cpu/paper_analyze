{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenjiazhai/miniconda3/envs/web/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import re\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    print(f'\\t创建 {path} 文件夹')\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    \n",
    "def download_pdf(save_dir, paper_title, file_name, pdf_url):\n",
    "    \"\"\"下载 PDF\n",
    "\n",
    "    Args:\n",
    "        save_path (_type_): _description_\n",
    "        paper_title (_type_): _description_\n",
    "        pdf_url (_type_): _description_\n",
    "    \"\"\"\n",
    "    response = requests.get(pdf_url)\n",
    "    bytes_io = io.BytesIO(response.content)\n",
    "    \n",
    "    with open(os.path.join(save_dir, f\"{file_name}\"), mode='wb') as f:\n",
    "        f.write(bytes_io.getvalue())\n",
    "        print(f'\\t{paper_title}.PDF 下载成功！')\n",
    "        \n",
    "def generate_file_name(address, title):\n",
    "    title = re.sub(f\"[{punctuation}]+\", ' ', title)\n",
    "    title = re.sub('\\s+', ' ', title)\n",
    "    return address + ' ' + title\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperRetrievor:\n",
    "    \"\"\"论文下载器。\n",
    "    \n",
    "    工作流程：\n",
    "    1. 遍历所有页，检验每一篇文献是否包含目标类别\n",
    "        - 是：爬取论文信息\n",
    "        - 不是：跳过\n",
    "    2. 保存论文信息到一个 csv 文件\n",
    "    3. 根据 csv 文件里的信息下载论文\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir) -> None:\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        create_folder(save_dir)\n",
    "        \n",
    "    def collect_paper_info(self, year, month, skip, show, target_category, download_papers=False):\n",
    "        query = 'https://arxiv.org/list/cs.{}/{}-{}?skip={}&show={}'\n",
    "        cache = {\n",
    "            'title':[],\n",
    "            'address':[],\n",
    "            'category':[],\n",
    "            'authors':[],\n",
    "            'abstract':[],\n",
    "            'comment':[],\n",
    "            'file_name':[]\n",
    "        }\n",
    "        stored_paper = set()\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = requests.get(query.format(cat, year + month, skip, show))\n",
    "        tree = html.fromstring(response.content)\n",
    "        target_value_xpath = '//*[@id=\"dlpage\"]/small[1]'\n",
    "        result = tree.xpath(target_value_xpath)[0].text_content().strip()\n",
    "        num_entries = int(re.search('total of (\\d+)', result).group(1))\n",
    "        print(f'{year} 年 {month} 月一共有 {num_entries} 篇论文，开始判断是否与目标类别相关...')\n",
    "        \n",
    "        for i in range(1, num_entries  // show + 2):\n",
    "            start_time_ = time.time()\n",
    "            if i != 1:\n",
    "                response = requests.get(query.format(year + month, skip, show))\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            print(f'第 {i} / {num_entries  // show + 1} 个页面的论文信息获取成功，解析中...')\n",
    "            \n",
    "            titles = soup.find_all('div', class_=\"list-title mathjax\")\n",
    "            titles = [t.select_one('span.descriptor').next_sibling.strip() for t in titles]\n",
    "            titles = [t.replace('  ', ' ') for t in titles]\n",
    "            \n",
    "            addresses = soup.find_all('dt')\n",
    "            addresses = [a.find_all('a')[2].attrs['href'].split('/')[-1] for a in addresses]\n",
    "            \n",
    "            categories = soup.find_all('span', class_='primary-subject')\n",
    "            categories = [c.contents[0] for c in categories]\n",
    "            categories = [re.search(r'\\((.+?)\\)', c).group(1) for c in categories]\n",
    "            \n",
    "            for title, address, cat in tqdm(zip(titles, addresses, categories)):\n",
    "                if cat != target_category:\n",
    "                    continue\n",
    "                if address in stored_paper:\n",
    "                    continue\n",
    "                try:\n",
    "                    file_name = generate_file_name(address, title)\n",
    "                    authors, abstract, comment = self.fetch_one_paper(year, month, title, cat, address, file_name, download_papers)\n",
    "                    cache['title'].append(title)\n",
    "                    cache['address'].append(address)\n",
    "                    cache['category'].append(cat)\n",
    "                    cache['authors'].append(authors)\n",
    "                    cache['abstract'].append(abstract)\n",
    "                    cache['comment'].append(comment)\n",
    "                    cache['file_name'].append(file_name+'.pdf')\n",
    "\n",
    "                    stored_paper.add(address)\n",
    "                except:\n",
    "                    print(f'\\t{address} 论文爬取出错')\n",
    "                \n",
    "            skip += show\n",
    "            print(f'第 {i} / {num_entries  // show + 1} 页爬取完毕，耗时 {(time.time() - start_time_)/60:.2f} min')\n",
    "            \n",
    "        df = pd.DataFrame(cache)\n",
    "        df.to_csv(os.path.join(self.save_dir, year + month + ' ' + target_category + '.csv'), index=False)\n",
    "        \n",
    "        print(f'{year} 年 {month} 月论文爬取 & 下载完成，共获取 {df.shape[0]} 篇论文 ，共耗时 {(time.time() - start_time)/60:.2f} min')\n",
    "    \n",
    "    \n",
    "    def fetch_one_paper(self, year, month, title, cat, address, file_name, download_papers):\n",
    "        web_page = 'https://arxiv.org/abs/{}'.format(address)\n",
    "        response = requests.get(web_page)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 找到包含作者信息的<div>标签\n",
    "        authors_div = soup.find('div', class_='authors')\n",
    "\n",
    "        # 提取作者信息\n",
    "        if authors_div:\n",
    "            # 使用正则表达式提取作者名字\n",
    "            authors_text = authors_div.get_text(strip=True)\n",
    "            authors_match = re.search(r'Authors:(.*)', authors_text)\n",
    "            \n",
    "            if authors_match:\n",
    "                authors = [author.strip() for author in authors_match.group(1).split(',')]\n",
    "                authors = '#'.join(authors)\n",
    "        else:\n",
    "            authors = ''        \n",
    "\n",
    "        if soup.find('span', class_='descriptor', string='Abstract:'):\n",
    "            abstract = soup.find('span', class_='descriptor', string='Abstract:').next_sibling.strip()\n",
    "        else:\n",
    "            abstract = ''\n",
    "        \n",
    "        if soup.find('td', class_='tablecell comments mathjax'):\n",
    "            comment = soup.find('td', class_='tablecell comments mathjax').string\n",
    "        else:\n",
    "            comment = ''\n",
    "\n",
    "        if download_papers:\n",
    "            create_folder(os.path.join(self.save_dir, year + month + ' ' + cat))\n",
    "            if not os.path.exists(os.path.join(self.save_dir, year + month + ' ' + cat, file_name + '.pdf')):\n",
    "                download_pdf(os.path.join(self.save_dir, year + month + ' ' + cat), address + ' ' + title, file_name + '.pdf', f'https://arxiv.org/pdf/{address}.pdf')\n",
    "        return authors, abstract, comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
