{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "def calculate_perplexity(sentence):\n",
    "    # Tokenize 输入句子\n",
    "    encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "\n",
    "    # 初始化变量\n",
    "    nlls = []\n",
    "    seq_len = input_ids.size(1)\n",
    "\n",
    "    # 遍历每个时间步，逐字计算困惑度\n",
    "    for i in range(1, seq_len):\n",
    "        # 当前时间步的输入序列\n",
    "        input_ids_step = input_ids[:, :i]\n",
    "        target_id = input_ids[:, i]  # 目标 token 是下一个字\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 获取模型输出 logits\n",
    "            outputs = model(input_ids_step)\n",
    "            logits = outputs.logits  # shape: (batch_size, seq_len, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]  # 只取最后一个 token 的预测分布\n",
    "\n",
    "            # 计算目标 token 的概率\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            target_prob = probs[:, target_id].squeeze()  # 取出目标 token 的概率\n",
    "\n",
    "            # 计算负对数似然 (NLL)\n",
    "            nll = -torch.log(target_prob)\n",
    "            nlls.append(nll)\n",
    "\n",
    "    # 计算整句困惑度\n",
    "    avg_nll = torch.stack(nlls).mean()\n",
    "    perplexity = torch.exp(avg_nll)\n",
    "\n",
    "    return perplexity.item()\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # 替换为任意支持的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "df = pd.read_csv('../../data/LongBench_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_contents = []\n",
    "ppls = []\n",
    "\n",
    "for con in df['context']:\n",
    "    sentences = sent_tokenize(con)\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    tmp_contents.append(sentences)\n",
    "    for s in sentences:\n",
    "        ppl = calculate_perplexity(s)\n",
    "        ppls.append(ppl)\n",
    "        \n",
    "threshold = np.percentile(ppls, 75)\n",
    "currect_idx = 0\n",
    "filtered_contents = []\n",
    "for con in tmp_contents:\n",
    "    filtered_sentences = []\n",
    "    for sen in con:\n",
    "        if ppls[current_idx] > threshold:\n",
    "            current_idx += 1\n",
    "            continue\n",
    "        filtered_sentences.append(sen)\n",
    "        current_idx += 1\n",
    "    filtered_contents.append(' '.join(filtered_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core import documents\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
    "chunks = splitter.create_documents(filtered_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../store', exist_ok=True)\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embed = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
    "vector_db = FAISS.load_local('../store/ppl_filter_langchain', embed, allow_dangerous_deserialization=True)\n",
    "retriever = vector_db.as_retriever(search_kwargs={'k':6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''You are a helpful assistant, please answer the following question based on the given content:\n",
    "\n",
    "Question:\n",
    "```\n",
    "{question}\n",
    "```\n",
    "\n",
    "Content:\n",
    "```\n",
    "{content}\n",
    "```\n",
    "\n",
    "Just give a simple answer, do not include any additional information or explaination.\n",
    "'''\n",
    "\n",
    "qa_instruction = '''\n",
    "You are an expert in information evaluation and critical thinking. Your task is to find the answer to a given question from a passage of text. You must carefully read every word and think through each step without overlooking any details. Your output should contain two fields: `Reasoning` and `Response`. In `Reasoning`, document your logical thought process in a clear, concise manner. If you find the answer, write it in the `Response` field; if not, try your best to guess one. The `Reasoning` should end with '*' to indicate completion.\n",
    "\n",
    "Objective: The task is to carefully analyze a passage of text to determine whether it contains the answer to a given question. The evaluation must be detailed, with clear reasoning, and identify the correct answer if present, or confirm its absence.\n",
    "\n",
    "You are provided with the following inputs:\n",
    "\n",
    "1. Context: {question}\n",
    "2. Question: {content}\n",
    "\n",
    "Based on these inputs, provide a step-by-step explanation to identify the correct answer from the content. If you cannot find the answer in the passage, try to guess the answer. Your response should only contain the answer itself. Do not explain, provide notes, or include any additional text, punctuation, or preposition (e.g., 'on', 'at'), or articles (e.g., 'a', 'an', 'the') unless absolutely necessary.\n",
    "\n",
    "Output format: \n",
    "\n",
    "-----\n",
    "SCHEMA\n",
    "-----\n",
    "\n",
    "{{\n",
    "    \"Reasoning\": \"Step-by-step reasoning explaining how the answer is inferenced to satisfy the question.\",\n",
    "    \"Response\": \"The answer itself, as simple as possible.\"\n",
    "}}\n",
    "\n",
    "-----\n",
    "\n",
    "1. Context: ```Pilotwings 64\\nPilotwings 64 (Japanese: パイロットウイングス64, Hepburn: Pairottouingusu Rokujūyon) is a video game for the Nintendo 64, originally released in 1996 along with the debut of the console. The game was co-developed by Nintendo and the American visual technology group Paradigm Simulation. It was one of three launch titles for the Nintendo 64 in Japan as well as Europe and one of two launch titles in North America. Pilotwings 64 is a follow-up to Pilotwings for the Super Nintendo Entertainment System (SNES), which was a North American launch game for its respective console in 1991. Also like that game, Pilotwings 64 received production input from Nintendo producer Shigeru Miyamoto.```\n",
    "2. Question: Who is a Japanese video game designer and producer, currently serving as the co-Representative Director of Nintendo, who gave production input to a video game for the Nintendo 64, originally released in 1996 along with the debut of the console?\n",
    "\n",
    "-----\n",
    "\n",
    "output:\n",
    "\n",
    "{{\n",
    "    \"Reasoning\": \"The context mentions that 'Pilotwings 64' was a video game released in 1996 for the Nintendo 64. The game received production input from Nintendo producer Shigeru Miyamoto. This aligns with the question, which asks for a Japanese video game designer and producer who gave production input to a Nintendo 64 game released in 1996. Additionally, Shigeru Miyamoto is well known as a prominent figure at Nintendo and is currently serving as the co-Representative Director of the company. Therefore, the content fully supports that Shigeru Miyamoto is the correct answer to the question.*\", \n",
    "    \"Response\": \"Shigeru Miyamoto\" \n",
    "}}\n",
    "\n",
    "-----\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'query': [], 'answer': [], 'predict': []}\n",
    "\n",
    "for d in tqdm(df.iterrows()):\n",
    "    q = d['query']\n",
    "    a = d['answer']\n",
    "    retrieval = ' '.join([r.page_content for r in retriever.invoke(q)])\n",
    "    # print(q)\n",
    "    # print(retrieval)\n",
    "    # break\n",
    "    # response = llm.invoke(qa_instruction.format(question=q, content=retrieval))\n",
    "    response = llm.invoke(prompt_template.format(question=q, content=retrieval))\n",
    "    # print(response)\n",
    "    # print(a)\n",
    "    # break\n",
    "    res['query'].append(q)\n",
    "    res['answer'].append(a)\n",
    "    res['predict'].append(response)\n",
    "    \n",
    "with open('../data/ppl_filtered_output.json', 'w') as f:\n",
    "    json.dump(res, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
