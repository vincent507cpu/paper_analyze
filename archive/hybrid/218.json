{"query": "what pretrained word embeddings were used?", "gold_answer": "Pretrained word embeddings  were not usedGloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16", "predict": "GloVe pretrained word embeddings and BioBERT model used in experiments.", "consistency": "0", "reasoning": "Answer A states that pretrained word embeddings were not used, while also listing GloVe and other embeddings, which creates confusion. Answer B clearly states that GloVe and BioBERT were used in experiments. Since Answer A contradicts the notion of using pretrained embeddings and Answer B confirms their use, the two answers are not semantically equivalent."}