{
    "query": "What are the baselines?",
    "gold_answer": "Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translationVanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-trainVanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. ",
    "predict": "The baselines are compared against other online models on datasets like BERT versus GloVe choice-only models. The results show that CRAFT outperforms all baselines in accuracy and F1 scores while maintaining competitive performance metrics across various tasks. Additionally, the study highlights the importance of rigorous baselines to help understand what constitutes a good model for conversational understanding.",
    "consistency": "0",
    "reasoning": "Answer A provides a detailed description of various baselines related to speech translation (ST), including specific types of pre-training and multi-task models. It focuses on the methodologies and architectures used in the context of ST. In contrast, Answer B discusses the comparison of baselines against online models, emphasizing performance metrics and the importance of baselines in evaluating model effectiveness. The two answers address different aspects of baselines: Answer A is technical and specific to ST methodologies, while Answer B is more general and focused on performance evaluation. Therefore, they are not semantically equivalent."
}